{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: The,POS Tag: DT\n",
      "Word: quick,POS Tag: JJ\n",
      "Word: brown,POS Tag: NN\n",
      "Word: for,POS Tag: IN\n",
      "Word: jumps,POS Tag: NNS\n",
      "Word: over,POS Tag: IN\n",
      "Word: the,POS Tag: DT\n",
      "Word: lazy,POS Tag: JJ\n",
      "Word: dogs,POS Tag: NNS\n",
      "Word: .,POS Tag: .\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "sentence = \"The quick brown for jumps over the lazy dogs.\"\n",
    "tokens= word_tokenize(sentence)\n",
    "pos_tags = pos_tag(tokens)\n",
    "for word, tag in pos_tags:\n",
    "    print(f\"Word: {word},POS Tag: {tag}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tokens are\n",
      "['The', 'quick', 'brown', 'for', 'jumps', 'over', 'the', 'lazy', 'dogs']\n",
      "Filtered tokens:\n",
      "['The', 'quick', 'brown', 'for', 'jumps', 'over', 'the', 'lazy', 'dogs']\n",
      "The tokens without stop words:\n",
      "['The', 'quick', 'brown', 'for', 'jumps', 'over', 'the', 'lazy', 'dogs']\n",
      "Stemmed Words\n",
      "['The', 'quick', 'brown', 'for', 'jump', 'over', 'the', 'lazy', 'dog']\n",
      "The text is in DEVANAGARI LETTER NA script.\n"
     ]
    }
   ],
   "source": [
    " \n",
    "def mytokenizer(corpus): \n",
    "    newcorpus=corpus+\"$\" \n",
    "    words=[] \n",
    "    tmp=\"\" \n",
    "    for i in range(len(sentence)): \n",
    "        if newcorpus[i] in [\" \",\"$\",\".\",\"\\n\"]: \n",
    "            if len(tmp)>0: \n",
    "                words.append(tmp) \n",
    "            tmp=\"\" \n",
    "        else: \n",
    "            tmp=tmp+sentence[i] \n",
    "    return words \n",
    " \n",
    "def text_filter(tokens): \n",
    "     #to remove punctuation from text \n",
    "     punc='''!()-[]{};:'\"\\,<>./?@#$%^&*_~।''' \n",
    " \n",
    "     clean_tokens=[w.strip(punc) for w in tokens] \n",
    "     # remove numbers from text \n",
    "     clean_tokens=[tx for tx in clean_tokens if not tx.isdigit()] \n",
    "     # remove numbers from alphanemeric words \n",
    "      \n",
    "     digits='''0123456789''' \n",
    " \n",
    "     clean_tokens=[w.strip(digits) for w in clean_tokens] \n",
    " \n",
    "     return clean_tokens \n",
    " \n",
    "stop_words=[\"i\",\"are\",\"we\",\"is\",\"to\",\"and\",\"will\",\"shall\",\"should\",\"they\",\"him\",\"he\",\"her\"] \n",
    "def remove_stopwords(tokens): \n",
    "    new_tokens=[w for w in tokens if w not in stop_words] \n",
    "    return new_tokens \n",
    " \n",
    "import re \n",
    "def my_stemmer(tokens): \n",
    "    stems = [ \n",
    "    # next line finds patterns and remove them from the string. \n",
    "    re.sub(r'less|ship|ing|les|ly|es|s|ity|ness|ed|ies', '', word)  \n",
    "    for word in tokens] \n",
    "    return stems \n",
    " \n",
    "import unicodedata \n",
    " \n",
    "def validate_script(text): \n",
    "    for char in text: \n",
    "        script = unicodedata.name(char) \n",
    "        if script != 'Common':  # Exclude common characters like spaces, punctuation, etc. \n",
    "            return script \n",
    "    return None  # If no significant characters were found sentence=\"Pointing to the close123 123 #@relations developed with India by the Awami League government, Bangladesh Information Minister Hasan Mahmud pointed out that the country's ties with its larger neighbour have always been affected and minorities have faced atrocities whenever the opposition BNP comes to power.\" \n",
    " \n",
    "tokens=mytokenizer(sentence) \n",
    " \n",
    "filtered_tokens=text_filter(tokens) \n",
    "new_filtered_tokens=remove_stopwords(filtered_tokens) \n",
    " \n",
    "stem_words=my_stemmer(new_filtered_tokens) \n",
    "print(\"The tokens are\") \n",
    "print(tokens) \n",
    "print(\"Filtered tokens:\") \n",
    "print(filtered_tokens) \n",
    "print(\"The tokens without stop words:\") \n",
    "print(new_filtered_tokens) \n",
    "print(\"Stemmed Words\") \n",
    "print(stem_words) \n",
    " \n",
    "text = \"नमस्ते\"  # Example text in Devanagari script (Hindi) \n",
    "script = validate_script(text) \n",
    "if script: \n",
    "    print(f\"The text is in {script} script.\") \n",
    "else: \n",
    "    print(\"Unable to determine the script.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\MRUH\n",
      "[nltk_data]     13\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\MRUH 13\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\MRUH\n",
      "[nltk_data]     13\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'word': 'happiness', 'lemma': 'happiness', 'derivational_morphemes': ['ness'], 'inflectional_morphemes': ['s']}\n",
      "{'word': 'running', 'lemma': 'running', 'derivational_morphemes': [], 'inflectional_morphemes': ['ing']}\n",
      "{'word': 'quickly', 'lemma': 'quickly', 'derivational_morphemes': ['ly'], 'inflectional_morphemes': []}\n",
      "{'word': 'teachers', 'lemma': 'teacher', 'derivational_morphemes': [], 'inflectional_morphemes': ['s']}\n",
      "{'word': 'reacted', 'lemma': 'reacted', 'derivational_morphemes': [], 'inflectional_morphemes': ['ed']}\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download resources (only needed once)\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to identify derivational and inflectional morphemes\n",
    "def find_morphemes(word):\n",
    "    lemma = lemmatizer.lemmatize(word)\n",
    "    \n",
    "    # Common English inflectional morphemes\n",
    "    inflectional_suffixes = ['s', 'es', 'ed', 'ing', 'er', 'est']\n",
    "    derivational_suffixes = ['ment', 'ness', 'ity', 'al', 'tion', 'ive', 'ize', 'ful', 'ous', 'ly']\n",
    "    \n",
    "    inflectional = [suffix for suffix in inflectional_suffixes if word.endswith(suffix)]\n",
    "    derivational = [suffix for suffix in derivational_suffixes if word.endswith(suffix)]\n",
    "    \n",
    "    return {\n",
    "        \"word\": word,\n",
    "        \"lemma\": lemma,\n",
    "        \"derivational_morphemes\": derivational,\n",
    "        \"inflectional_morphemes\": inflectional\n",
    "    }\n",
    "\n",
    "# Example words\n",
    "words = [\"happiness\", \"running\", \"quickly\", \"teachers\", \"reacted\"]\n",
    "\n",
    "# Find morphemes for each word\n",
    "for word in words:\n",
    "    print(find_morphemes(word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next word after 'the': cat\n",
      "Next word after 'cat': .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\MRUH\n",
      "[nltk_data]     13\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter, defaultdict\n",
    "import random\n",
    "\n",
    "# Download necessary NLTK resources (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Sample corpus (you can replace this with a larger dataset)\n",
    "corpus = \"The cat sat on the mat. The dog barked at the cat. The cat ran away. The dog chased the cat.\"\n",
    "\n",
    "# Tokenize words\n",
    "tokens = nltk.word_tokenize(corpus.lower())  # Convert to lowercase for consistency\n",
    "\n",
    "# Generate bigrams (2-gram model)\n",
    "bigrams = list(ngrams(tokens, 2))\n",
    "\n",
    "# Count frequency of bigrams\n",
    "bigram_freq = Counter(bigrams)\n",
    "\n",
    "# Create a dictionary of next word probabilities with Laplace Smoothing\n",
    "word_followers = defaultdict(lambda: defaultdict(lambda: 1))  # Laplace smoothing (add-1)\n",
    "\n",
    "for w1, w2 in bigram_freq:\n",
    "    word_followers[w1][w2] += bigram_freq[(w1, w2)]\n",
    "\n",
    "# Normalize probabilities\n",
    "for w1 in word_followers:\n",
    "    total_count = sum(word_followers[w1].values())\n",
    "    for w2 in word_followers[w1]:\n",
    "        word_followers[w1][w2] /= total_count  # Convert to probability\n",
    "\n",
    "# Function to predict the next word\n",
    "def predict_next_word(word):\n",
    "    if word in word_followers:\n",
    "        return max(word_followers[word], key=word_followers[word].get)  # Choose most probable next word\n",
    "    else:\n",
    "        return \"UNKNOWN\"  # If word is not in the corpus\n",
    "\n",
    "# Example predictions\n",
    "input_word = \"the\"\n",
    "predicted_word = predict_next_word(input_word)\n",
    "print(f\"Next word after '{input_word}': {predicted_word}\")\n",
    "\n",
    "input_word = \"cat\"\n",
    "predicted_word = predict_next_word(input_word)\n",
    "print(f\"Next word after '{input_word}': {predicted_word}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
